# ğŸ’¤ Bocejos LSTM â€“ DetecÃ§Ã£o de SonolÃªncia com LSTM

![GIF](./documentos/GIF_GITHUB.gif)

## ğŸ› ï¸ Tecnologias Utilizadas

- Python 3.9  
- TensorFlow  
- ONNX  
- MediaPipe  
- OpenCV  
- NumPy  
- Conda (ambientes virtuais)

---

Este projeto utiliza modelos LSTM para classificar estados de sonolÃªncia com base em keypoints extraÃ­dos de vÃ­deos. As classes detectadas sÃ£o:

- **0 - Alert**: Olhos abertos e postura estÃ¡vel  
- **1 - Yawning**: Boca se abrindo com expressÃ£o clara  
- **2 - Microsleep**: Olhos fechando por tempo prolongado  

## ğŸ“ Estrutura do RepositÃ³rio

```bash
Bocejos LSTM/
â”‚
â”œâ”€â”€ create_labels/                        # Scripts e dados para criaÃ§Ã£o de labels e treinamento do modelo
â”‚   â”œâ”€â”€ code/                             # CÃ³digo-fonte principal
â”‚   â”‚   â”œâ”€â”€ documentos/                   # Modelo LSTM salvo em formato ONNX
â”‚   â”‚   â”‚   â””â”€â”€ model_lstm.onnx
â”‚   â”‚   â”œâ”€â”€ model_tensorflow/            # Modelos e scripts em TensorFlow (base para exportaÃ§Ã£o ONNX)
â”‚   â”‚   â”‚   â”œâ”€â”€ extract_keypoints-kaggle.py              # Extrai keypoints do dataset Kaggle (sem mÃ£os)
â”‚   â”‚   â”‚   â”œâ”€â”€ extract_keypoints-kaggle-hands.py        # Extrai keypoints do dataset Kaggle (com mÃ£os)
â”‚   â”‚   â”‚   â”œâ”€â”€ extract_keypoints-videos.py              # Extrai keypoints de vÃ­deos locais (sem mÃ£os)
â”‚   â”‚   â”‚   â”œâ”€â”€ extract_keypoints-videos-hands.py        # Extrai keypoints de vÃ­deos locais (com mÃ£os)
â”‚   â”‚   â”‚   â”œâ”€â”€ train_lstm.py                            # Treinamento do modelo LSTM (sem mÃ£os)
â”‚   â”‚   â”‚   â””â”€â”€ train_lstm-hands.py                      # Treinamento do modelo LSTM (com mÃ£os)
â”‚   â”œâ”€â”€ datasets/                        # Datasets utilizados no projeto (nÃ£o incluÃ­dos por tamanho)
â”‚   â”‚   â”œâ”€â”€ FL3D - Kaggle
â”‚   â”‚   â”œâ”€â”€ DMD â€“ Driving Monitoring Dataset
â”‚   â”‚   â””â”€â”€ Videos Fadiga - Pessoal
â”‚   â”œâ”€â”€ jsons/                           # JSONs com keypoints extraÃ­dos
â”‚   â”‚       â”œâ”€â”€ FL3D - Kaggle3.json
â”‚   â”‚       â”œâ”€â”€ DMD â€“ Driving Monitoring Dataset.json
â”‚   â”‚       â”œâ”€â”€ Videos pessoais para fine-tuning do modelo.json
â”‚   â”‚       â””â”€â”€ Other/
â”‚   â”œâ”€â”€ labels_explanation.txt          # ExplicaÃ§Ã£o das labels utilizadas para classificaÃ§Ã£o
â”‚   â”œâ”€â”€ save_to_onnx.txt                # Comando de exemplo para exportar o modelo para formato ONNX
â”‚
â”œâ”€â”€ documentos/                         # Pasta genÃ©rica para arquivos auxiliares
â”œâ”€â”€ run.py                              # Executa o modelo em tempo real via webcam (sem mÃ£os)
â”œâ”€â”€ run-hands.py                        # Executa o modelo em tempo real via webcam (com mÃ£os)
â”œâ”€â”€ requirements.txt                    # DependÃªncias para treinamento e execuÃ§Ã£o do modelo padrÃ£o
â”œâ”€â”€ requirements_onnx.txt               # DependÃªncias adicionais para exportaÃ§Ã£o do modelo para ONNX
```

---

## â„¹ï¸ ObservaÃ§Ã£o sobre os arquivos com -hands no nome:
Os arquivos que possuem -hands no nome sÃ£o versÃµes alternativas dos scripts que incluem keypoints das mÃ£os durante a extraÃ§Ã£o dos dados e no treinamento do modelo. Essa variaÃ§Ã£o pode ser Ãºtil para cenÃ¡rios onde o movimento das mÃ£os Ã© relevante para a detecÃ§Ã£o de bocejos ou fadiga.

Por exemplo:

`train_lstm.py`: treinar modelo sem considerar mÃ£os
`train_lstm-hands.py`: treinar modelo considerando keypoints das mÃ£os

`run.py`: inferÃªncia em tempo real sem mÃ£os
`run-hands.py`: inferÃªncia em tempo real com mÃ£os

Escolha os scripts com ou sem hands dependendo da abordagem desejada para o seu caso de uso.

---

## ğŸ“¦ Como Instalar e Executar

### 1ï¸âƒ£ Ambiente principal (treinar modelo e executar via webcam)

```bash
conda create -n lstm_env python=3.9 -y
conda activate lstm_env

pip install -r requirements.txt
```

### 2ï¸âƒ£ Ambiente ONNX (converter modelo para ONNX)

```bash
conda create -n lstm_env_onnx python=3.9 -y
conda activate lstm_env_onnx

pip install -r requirements_onnx.txt
```

---

## ğŸš€ Como Usar

### 1. Extrair keypoints:
- Para o dataset do **Kaggle**:
  ```bash
  python extract_keypoints-kaggle.py
  ```
- Para vÃ­deos locais:
  ```bash
  python extract_keypoints-videos.py
  ```

### 2. Treinar o modelo:
```bash
python train_lstm.py
```
O modelo serÃ¡ salvo nas pastas `model_tensorflow/` e `code/documentos/`.

### 3. Converter para ONNX:
Execute o comando dentro de `save_to_onnx.txt` no terminal, dentro do ambiente `lstm_env_onnx`.

### 4. Rodar o modelo em tempo real (via webcam):
```bash
python run.py
```

---

## ğŸ§  Labels Classificadas

| Classe      | DescriÃ§Ã£o                                 | Valor |
|-------------|--------------------------------------------|--------|
| `alert`     | Olhos abertos e postura estÃ¡vel           | `0`    |
| `yawning`   | Boca se abrindo com expressÃ£o clara       | `1`    |
| `microsleep`| Olhos fechando por tempo prolongado       | `2`    |

---

## ğŸ“Œ ObservaÃ§Ãµes

- Os datasets utilizados estÃ£o na pasta `datasets/`, mas **nÃ£o foram incluÃ­dos no repositÃ³rio** devido ao tamanho.  
- Os arquivos JSON com os keypoints extraÃ­dos (que podem ser usados para treinamento na mesma pasta) estÃ£o em `jsons/`.

## Resultados e DecisÃµes

Testamos dois modelos: um usando apenas pontos faciais e outro incluindo tambÃ©m os keypoints das mÃ£os. O modelo **sem mÃ£os** teve desempenho melhor. A inclusÃ£o das mÃ£os piorou os resultados, principalmente porque as mÃ£os muitas vezes tampam a boca, e como temos poucos dados, o modelo acaba interpretando qualquer mÃ£o na frente como bocejo.

### Comparativo:

- **Modelo sem mÃ£os:**

  ![Sem mÃ£os](./documentos/lstm_euclidean_normalized.png)

- **Modelo com mÃ£os:**

  ![Com mÃ£os](./documentos/lstm_hands_euclidean_normalized.png)

Ambos foram treinados com ~86.000 frames (20% para teste). O modelo sem mÃ£os teve um equilÃ­brio melhor entre precisÃ£o e recall, especialmente para a classe "Bocejando", que era a mais sensÃ­vel a erros.

---

## NormalizaÃ§Ã£o Euclidiana

A melhor forma de prÃ©-processamento que encontramos foi a **normalizaÃ§Ã£o euclidiana** dos keypoints faciais. Basicamente, pegamos a distÃ¢ncia entre os olhos e usamos ela para normalizar a posiÃ§Ã£o dos pontos, assim o modelo nÃ£o Ã© afetado por variaÃ§Ãµes de distÃ¢ncia da pessoa atÃ© a cÃ¢mera.

FÃ³rmula usada:

```python
def euclidean(p1, p2):
    return ((p1.x - p2.x)**2 + (p1.y - p2.y)**2)**0.5
```

Keypoints de referÃªncia:
```python
left_eye_idx = 33
right_eye_idx = 263
```

---

## Arquitetura do Modelo

```python
model = Sequential()
model.add(Masking(mask_value=0.0, input_shape=(X_train.shape[1], X_train.shape[2])))
model.add(LSTM(64, return_sequences=False))
model.add(Dense(64, activation='relu'))
model.add(Dense(3, activation='softmax'))
```

Foi usado `Masking` para ignorar padding com zeros, uma camada LSTM para processar a sequÃªncia dos frames e duas densas para a classificaÃ§Ã£o.

---

## Treinamento

O modelo foi treinado com early stopping ativado, para evitar overfitting:

```python
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
```

```python
model.fit(X_train, y_train,
          epochs=50,
          batch_size=32,
          validation_data=(X_test, y_test),
          # class_weight=class_weights,
          callbacks=[early_stop])
```

---

## Dados Utilizados

- **FL3D (Kaggle)**: 49 GIFs de bocejos
- **6 vÃ­deos adicionais** (Google Drive): gravaÃ§Ãµes com eventos de fadiga

Os vÃ­deos foram convertidos em frames e os keypoints foram extraÃ­dos com MediaPipe.

---
